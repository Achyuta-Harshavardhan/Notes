Cross Validation:
1. Leave One Out CV (LOOCV)---> (Low bias and high computation)
2. K fold CV
3. Stratified cross validation --> no of instance in each class is in proper proportion
4. Time series CV

Sensitiviry: TP/TP+FN 
Specificity: TN/TN+FP

Sensitivity need to be high if positive model prediction is important
Specificity need to be high if negative model prediction is important

Bias is the inability of the machine learning model to capture the true relationship.
Varience is the difference in fits between the data sets

True positive rate = sensitivity
False Positive rate = 1- specificity

ROC summarizes all of the confusion matrices that each threshold prodeuced
(It is the graph btw true positive rate and false positive rate) --> It is used to identify the best threshold value
AUC is used to decide wich method is better

Linear Regression:
1. we need to minimize the square of the distance btw the observed values and the line.

































Decision Tree:

The goal of Decision tree is to identify the decision rules

Basic Assumption: No Basic Assumption
Feature

yes/No:
1. Calculate the Gini impurity scores
2. if the node itself has the lowest score, then there is no point in seperating further and it becomes the leaf node
3. if seperating the data results in an improvement, then pick the seperation with the lowest impurity value

Numeric:
1. sort all the values in ascending order
2. calculate the average value for all the adjacent records
3. calculate the gini impurity for each of the average value

Ranked/Multiple choice:
1. If there are four ranks we need to calculate the gini impurity for rank 1,2 and 3.
2. for multiple choices we need to calculate giniimpurity for each one as well as each possible combination.

Gini impurity is used because it is computationally efficient

Random Forest:

